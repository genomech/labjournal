Абстрактный 

Клиническая генетика играет важную роль в системе здравоохранения, обеспечивая окончательный диагноз многих редких синдромов. Он также может влиять на генетическую профилактику, прогноз заболевания и помогать в выборе лучших вариантов ухода / лечения для пациентов. Секвенирование следующего поколения (NGS) изменило клиническую генетику, сделав возможным анализировать сотни генов с беспрецедентной скоростью и по более низкой цене по сравнению с обычным секвенированием по Сэнгеру. Несмотря на растущее количество литературы, посвященной NGS в клинических условиях, этот обзор направлен на восполнение пробела, существующего среди (био) информатиков, молекулярных генетиков и клиницистов, путем представления общего обзора технологии и рабочего процесса NGS. Во-первых, мы рассмотрим текущие платформы NGS, сосредоточив внимание на двух основных платформах, Illumina и Ion Torrent, и обсудим основные сильные и слабые стороны, присущие каждой платформе. Затем анализируются аналитические биоинформатические конвейеры NGS, при этом особое внимание уделяется алгоритмам, обычно используемым для генерации данных процесса и анализа вариантов последовательности. Наконец, основные проблемы, связанные с биоинформатикой NGS, рассматриваются в перспективе для будущих разработок. Даже с огромными достижениями, достигнутыми в технологии NGS и биоинформатике, все еще требуются дальнейшие улучшения биоинформатических алгоритмов для борьбы со сложными и генетически гетерогенными нарушениями. 
Ключевые слова: биоинформатика, клиническая генетика, высокопроизводительные данные, конвейер NGS, платформы NGS. 
Идти к: 
1. Введение 

В настоящее время генетика имеет чрезвычайно важное значение для медицинской практики, поскольку она обеспечивает окончательный диагноз многих клинически неоднородных заболеваний. Следовательно, он позволяет более точно прогнозировать заболевание и дает рекомендации по выбору наилучших возможных вариантов лечения пораженных пациентов. Большая часть его нынешнего потенциала проистекает из способности исследовать геном человека на разных уровнях, от хромосомных до одноосновных изменений. 

Пионерские работы Пола Берга [1], Фредерика Сэнгера [2] и Уолтера Гилберта [3] по секвенированию ДНК сделали возможным ряд достижений в этой области, а именно разработку метода, открывшего совершенно новые возможности для анализа ДНК, метода Сэнгера. Технология секвенирования с окончанием цепи, наиболее известная как секвенирование по Сэнгеру [4]. Дальнейшие технологические разработки ознаменовали рост секвенирования ДНК, что позволило запустить в 1986 г. первый автоматический секвенатор ДНК (ABI PRISM AB370A), который позволил разработать геном человека в течение следующего десятилетия [5]. С тех пор прогресс продолжался, и, по сути, за последние два десятилетия решительные шаги, а именно в нанотехнологиях и информатике, внесли свой вклад в новое поколение методов секвенирования (рис. 1). 
Внешний файл, содержащий изображение, иллюстрацию и т. Д. Имя объекта: jcm-09-00132-g001.jpg 
Открыть в отдельном окне 
Рисунок 1 

Хронология секвенирования ДНК. Некоторые из самых революционных и замечательных событий в области секвенирования ДНК. НГ - следующее поколение; ПЦР - полимеразная цепная реакция; SMS - секвенирование отдельной молекулы; SeqLL - последовательность нижнего предела. 

Эти новые подходы направлены на то, чтобы дополнить и в конечном итоге заменить секвенирование по Сэнгеру. Эта технология в совокупности называется секвенированием следующего поколения (NGS) или массово-параллельным секвенированием (MPS), что часто является зонтиком для обозначения широкого разнообразия подходов. Благодаря этой технологии можно генерировать огромные объемы данных за один запуск инструмента более быстрым и экономичным способом, передавая параллельный анализ нескольких генов или даже всего генома. В настоящее время рынок NGS расширяется, и прогнозируется, что к 2025 году мировой рынок достигнет 21,62 миллиарда долларов США, увеличившись примерно на 20% с 2017 по 2025 год (BCC Research, 2019). Таким образом, в настоящее время на рынке NGS присутствует несколько брендов, среди которых Illumina, Ion Torrent (Thermo Fischer Scientific), BGI Genomics, PacBio и Oxford Nanopore Technologies входят в число ведущих компаний по секвенированию. Все они предлагают разные стратегии решения одной и той же проблемы, а именно - массового получения данных секвенирования. Для простоты, хотя и не полностью согласованной классификации в литературе, можно считать, что секвенирование второго поколения основано на массивной параллельной и клональной амплификации молекул (полимеразная цепная реакция (ПЦР)) [6]; тогда как секвенирование третьего поколения основано на секвенировании одной молекулы без предварительной стадии клональной амплификации [7,8,9]. 

В этом обзоре будет представлен упрощенный обзор различных шагов, связанных с биоинформатическим анализом данных NGS, и даны некоторые сведения об основных алгоритмах анализа этих данных. 
Идти к: 
2. Библиотека NGS 

В NGS библиотека определяется как коллекция фрагментов ДНК / РНК, которая представляет либо весь геном / транскриптом, либо целевую область. Каждая платформа NGS имеет свои особенности, но, говоря простым языком, подготовка библиотеки NGS начинается с фрагментации исходного материала, затем адаптеры последовательностей подключаются к фрагментам, чтобы обеспечить обогащение этих фрагментов. Хорошая библиотека должна обладать большой чувствительностью и специфичностью. Это означает, что все интересующие фрагменты должны быть одинаково представлены в библиотеке и не должны содержать случайных ошибок (неспецифических продуктов). Однако легче сказать, чем сделать, поскольку области генома не одинаково подвержены секвенированию, что затрудняет создание чувствительной и специфической библиотеки [10]. 

Первым шагом к подготовке библиотек в большинстве рабочих процессов NGS является фрагментация нуклеиновой кислоты. Фрагментация может производиться физическими или ферментативными методами [11,12]. Физические методы включают акустический сдвиг, обработку ультразвуком и гидродинамический сдвиг. Ферментативные методы включают расщепление ДНКазой I или фрагментазой. Knierim и соавторы сравнили методы ферментативной и физической фрагментации и обнаружили аналогичные выходы, показав, что выбор между физическим или ферментативным методом зависит только от экспериментального плана или внешних факторов, таких как лабораторные условия [13]. 

После того, как исходная ДНК фрагментирована, к этим фрагментам подключаются адаптеры. Адаптеры вводятся для создания известных начала и окончания случайных последовательностей, позволяющих процесс упорядочивания. Была разработана альтернативная стратегия, которая сочетает в себе фрагментацию и лигирование адаптера за один шаг, что делает процесс проще, быстрее и требует меньшего количества вводимых образцов. Этот процесс известен как тегирование и основан на технологии на основе транспозонов [14]. 

После фрагментации нуклеиновой кислоты фрагменты выбирают в соответствии с желаемым размером библиотеки. Это ограничено либо типом инструмента NGS, либо конкретным приложением для секвенирования. Секвенсоры с коротким считыванием, такие как Illumina и Ion Torrent, показывают лучшие результаты, когда библиотеки ДНК содержат более короткие фрагменты аналогичного размера. Фрагменты Illumina длиннее, чем в Ion Torrent, и могут достигать 1500 баз в длину [11], тогда как в Ion Torrent фрагменты могут достигать 400 баз в длину [15]. Напротив, секвенаторы с длинным считыванием, такие как PacBio RS II [16], имеют тенденцию производить сверхдлинные чтения путем полного секвенирования фрагмента ДНК. Оптимальный размер библиотеки также ограничен приложением для секвенирования. Для полногеномного секвенирования более длинные фрагменты предпочтительны, в то время как для секвенирования РНК-seq и экзома возможны более мелкие фрагменты, поскольку большинство экзонов человека имеют длину менее 200 пар оснований [17]. 

Затем требуется этап обогащения, на котором количество целевого материала увеличивается в библиотеке, подлежащей секвенированию. Когда требуется исследовать только часть генома для исследовательских или клинических целей, это называется целевыми библиотеками. В основном, для таких целевых подходов обычно используются два метода: секвенирование на основе гибридизации и секвенирование на основе ампликонов [18,19]. В методе гибридного захвата на стадии фрагментации фрагментированные молекулы специфически гибридизуются с фрагментами ДНК, комплементарными интересующим целевым областям. Это можно сделать с помощью различных методов, таких как технология микрочипов или использование биотинилированных олигонуклеотидных зондов [20], целью которых является физический захват и выделение интересующих последовательностей. Двумя хорошо известными примерами коммерческих решений для подготовки библиотек на основе гибридных методов захвата являются SureSelect (Agilent Technologies) и SeqCap (Roche). 

Что касается методов на основе ампликонов, они основаны на разработке синтетических олигонуклеотидов (или зондов) с последовательностью, комплементарной фланкирующим областям целевой ДНК, подлежащей секвенированию. HaloPlex (Agilent Technologies) и AmpliSeq (Ion Torrent) - два примера коммерческих решений для подготовки библиотек, основанных на стратегиях на основе ампликонов. Методы на основе ампликонов имеют ограничения, присущие амплификациям ПЦР, такие как систематическая ошибка, дублирование ПЦР, конкуренция праймеров и неоднородная амплификация целевых областей (из-за вариации содержания GC) [21]. Было показано, что гибридные методы захвата превосходят методы на основе ампликонов, обеспечивая гораздо более однородный охват и глубину, чем анализы ампликонов [19]. Однако недостатком методов гибридизации является более высокая стоимость из-за специфики метода (стоимость зондов, экспериментальный план, программное обеспечение и т. Д.), И они требуют больше времени, чем подходы с использованием ампликона. Следовательно, было предпринято несколько попыток преодолеть ограничения ПЦР. Одна многообещающая стратегия - это уникальные молекулярные идентификаторы (UMI), которые представляют собой короткие молекулы ДНК, которые лигируются с фрагментами библиотеки [22]. Эти UMI имеют случайную последовательность последовательностей, которая гарантирует, что каждый фрагмент с UMI уникален в вашей библиотеке. Это позволяет после обогащения ПЦР, дубликаты ПЦР могут быть найдены путем поиска неуникальных комбинаций фрагмент-UMI, в то время как реальный биологический дублированный будет содержать эти последовательности UMI [23,24]. 
Идти к: 
3. Платформы NGS 
3.1. Платформы секвенирования второго поколения 

Платформы второго поколения относятся к группе технологий секвенирования циклических массивов (см. Обзор [6]). Основной рабочий процесс для платформ второго поколения включает подготовку и амплификацию библиотек (приготовленных из образцов ДНК / РНК), клональное расширение, секвенирование и анализ. Две наиболее известные компании по секвенированию платформ для секвенирования второго поколения - это Illumina и Ion Torrent. 

Illumina - широко известная американская компания, которая коммерциализирует несколько интегрированных систем для анализа генетических вариаций и биологических функций, которые могут применяться во многих биологических системах от сельского хозяйства до медицины. Процесс секвенирования Illumina основан на концепции секвенирования путем синтеза (SBS) с захватом на твердой поверхности отдельных молекул с последующей мостовой ПЦР, которая позволяет его амплифицировать в небольшие кластеры идентичных молекул. Вкратце, ДНК / кДНК фрагментируется, и к обоим концам фрагментов добавляются адаптеры. Затем каждый фрагмент прикрепляется к поверхности проточной кюветы с помощью олигонуклеотидов на поверхности, которые имеют нуклеотидную последовательность, комплементарную адаптерам, что позволяет проводить гибридизацию и последующую амплификацию моста, образуя двухцепочечный мостик. Затем его денатурируют, чтобы получить однонитевые шаблоны, которые прикрепляются к подложке. Этот процесс непрерывно повторяется и генерирует несколько миллионов плотных кластеров двухцепочечной ДНК в каждом канале проточной ячейки. Затем можно выполнить секвенирование путем добавления к матрице (на проточной ячейке) одного меченого комплементарного дезоксинуклеотидтрифосфата (dNTP), который служит «обратимым терминатором». Флуоресцентный краситель идентифицируется с помощью лазерного возбуждения и визуализации, а затем он ферментативно расщепляется, чтобы обеспечить следующий раунд включения. В Illumina необработанные данные - это измерения интенсивности сигнала, обнаруженные во время каждого цикла [25]. В целом эта технология считается высокоточной и надежной, но имеет некоторые недостатки. Например, во время процесса секвенирования некоторые красители могут потерять активность и / или может произойти частичное перекрытие между спектрами излучения флуорофоров, что ограничивает потребность основания в платформе Illumina [26,27]. 

Ion Torrent, главный конкурент Illumina, использует особую концепцию последовательности, называемую полупроводниковой секвенированием. Метод секвенирования в Ion Torrent основан на изменении pH, вызванном высвобождением ионов водорода (H +) во время полимеризации ДНК. В Ion Torrent вместо того, чтобы прикрепляться к поверхности проточной ячейки, фрагментированные молекулы связываются с поверхностью шариков и амплифицируются с помощью эмульсионной ПЦР, генерируя шарики с клонально амплифицированной целевой молекулой (молекулой / шариком). Затем каждую гранулу помещают в микролунку на микросхеме матрицы полупроводниковых датчиков, называемую комплементарным кристаллом металл-оксид-полупроводник (CMOS) [28,29]. Каждый раз, когда полимераза включает комплементарный нуклеотид в растущую цепь, высвобождается протон, вызывая изменения pH в растворе, которые обнаруживаются ионным датчиком, встроенным в чип. Эти изменения pH преобразуются в сигналы напряжения, которые затем используются для определения основания. У этой технологии также есть ограничения, при этом основным источником ошибок секвенирования является возникновение гомополимерных растяжек матрицы. Во время секвенирования произойдет многократное включение одного и того же основания в каждую цепь, что приведет к высвобождению более высокой концентрации H + в одном потоке. Повышенный сдвиг в pH генерирует больший сигнал включения, указывая системе, что было включено более одного нуклеотида. Однако для более длинных гомополимеров система неэффективна, что делает их количественное определение неточным [29,30]. 
3.2. Секвенсоры третьего поколения 

Технология NGS 3-го поколения дала возможность обойти общие и трансверсальные ограничения методов на основе ПЦР, такие как неправильное включение нуклеотидов полимеразой, образование химер и выпадение аллелей (преимущественная амплификация одного аллеля), вызывающее искусственный вызов гомозиготности. Первым коммерческим секвенатором 3-го поколения была платформа Helicos Genetic Analysis System [31,32]. В отличие от секвенаторов второго поколения, здесь ДНК просто разрезали, образуя хвосты поли-А и гибридизовали с поверхностью проточной клетки, содержащей нуклеотиды олиго-Т. Реакция секвенирования происходит путем включения меченого нуклеотида, который фиксируется камерой. С помощью этой технологии каждая нить уникально и независимо секвенируется [31]. Однако он был относительно медленным и дорогим и недолго устоял на рынке. 

В 2011 году компания Pacific Biosystems представила концепцию секвенирования одиночных молекул в реальном времени (SMRT) с секвенатором PacBio RS II [33]. Кроме того, эта технология позволяет выполнять последовательность длинных считываний (со средней длиной считывания до 30 КБ) [33]. Отдельные ДНК-полимеразы прикрепляются к лункам волновода нулевой моды (ZMW), которые представляют собой наноотверстия, в которые можно напрямую поместить одну молекулу фермента ДНК-полимеразы [33]. Одиночная молекула ДНК используется в качестве матрицы для включения полимеразы флуоресцентно меченых нуклеотидов. Каждая основа имеет различный флуоресцентный краситель, тем самым испуская сигнал из ZMW. Детектор считывает флуоресцентный сигнал и на основе цвета обнаруженного сигнала идентифицирует сигнал. Затем добавление основания приводит к расщеплению флуоресцентной метки полимеразой. Замкнутая кольцевая однониточная ДНК (оцДНК) используется в качестве матрицы (называемой SMRTbell) и может быть секвенирована несколько раз для обеспечения более высокой точности [33]. Кроме того, эта технология обеспечивает сборку de novo и прямое обнаружение гаплотипов; высокая согласованная точность и позволяет эпигенетическую характеристику (прямое обнаружение модификаций оснований ДНК с разрешением в одно основание) [7,34]. Первые секвенсоры, использующие технологию SMRT, столкнулись с недостатками ограниченной высокой пропускной способности, более высокой стоимости и более высокой частоты ошибок. Однако для преодоления этих ограничений были достигнуты значительные улучшения. Совсем недавно PacBio запустила систему Sequel II, которая утверждает, что сокращает затраты и сроки проекта по сравнению с предыдущими версиями, с высокоточным индивидуальным длинным чтением (чтение HiFi, до 175 кб) [35]. Используя систему PacBio, Меркер и его сотрудники заявили, что они первыми продемонстрировали успешное применение длинночитываемого секвенирования генома для выявления патогенного варианта у пациента с менделевской болезнью, что позволяет предположить, что эта технология имеет значительный потенциал для идентификации болезни. -вызывающие структурные изменения [36]. 

Второй подход к секвенированию одиночных молекул был коммерциализирован Oxford Nanopore Technologies (некоторые авторы называют его четвертым поколением) под названием MinION, коммерчески доступным в 2015 году [37]. Этот секвенатор не полагается на SBS, а вместо этого полагается на электрические изменения тока, когда каждый нуклеотид (A, C, T и G) проходит через нанопору [38]. При секвенировании нанопор используется электрофорез для транспортировки неизвестного образца через небольшое отверстие, затем ионный ток проходит через нанопоры, и ток изменяется по мере прохождения оснований через поры в различных комбинациях. Эта информация позволяет идентифицировать каждую молекулу и выполнить секвенирование [39]. В мае 2017 года Oxford Nanopore Technologies выпустила GridION Mk1, гибкое настольное устройство для секвенирования и анализа, которое предлагает в реальном времени, долгое считывание, высокоточное секвенирование ДНК и РНК. Он был разработан, чтобы позволить проводить до пяти экспериментов одновременно или индивидуально, с простой подготовкой библиотеки, что позволяет генерировать до 150 Гб данных во время прогона [40]. В этом году были запущены новые достижения с системой PromethION 48, которая предлагает 48 проточных ячеек, и каждая проточная ячейка позволяет одновременно обрабатывать до 3000 нанопор, что может обеспечить выход до 7,6 ТБ за 72 часа. Более длинные чтения имеют первостепенное значение для выявления повторяющихся элементов и сложных последовательностей, таких как мобильные элементы, сегментарные дупликации и теломерные / центромерные области, с которыми трудно справиться с помощью коротких чтений [41]. Эта технология уже позволила идентифицировать, аннотировать и характеризовать десятки тысяч структурных вариантов (SV) из генома человека [42]. Хотя точность секвенирования нанопор еще не сопоставима с точностью секвенирования с коротким считыванием (например, платформы Illumina заявляют о точности 99,9%), обновления происходят постоянно, и постоянные разработки направлены на расширение диапазона геномов и дальнейшее повышение точности технологии. [37,43]. 

Компания 10X Genomics была основана в 2012 году и предлагает инновационные решения от анализа отдельных клеток до комплексного анализа SV и анализа вариантов числа копий. В 2016 году 10X Genomics запустила инструмент Chromium, который включает гелевые шарики в эмульсии (GEM). В этой технологии в каждую гранулу геля вводят миллионы уникальных олигонуклеотидных последовательностей и смешивают с образцом (который может быть ДНК с высокой молекулярной массой (HMW), отдельными клетками или ядрами). Затем гелевые шарики с образцами добавляют к раствору масло-поверхностно-активное вещество для создания гелевых шариков в эмульсии (GEM). GEM действуют как отдельные реакционные везикулы, в которых растворяются шарики геля, и на образец наносится штрих-код для создания коротких последовательностей со штрих-кодом [44]. Преимущество технологии GEM заключается в том, что она сокращает время, количество исходного материала и затраты [45,46]. Для анализа структурных вариантов эти библиотеки короткого чтения реконструируются с помощью вычислений, чтобы иметь возможность выполнять последовательности генома гаплотипа в мегабазном масштабе, используя небольшие количества входной ДНК. Чжэн с соавторами показали, что эта технология позволяет фазирование связанного считывания SV отличает истинные SV от ложных предсказаний и может применяться для сборки генома de novo, переназначения сложных участков генома, обнаружения редких аллелей. и выяснение сложных структурных перестроек [45]. Хромовая система также предлагает одноклеточный геном и профилирование транскрипции, иммунное профилирование и анализ доступности хроматина при одноклеточном разрешении с низкой частотой ошибок и высокой пропускной способностью [47, 48]. Таким образом, открываются захватывающие новые приложения, особенно в области разработки новых методов исследования эпигенетики [49], сборки генома de novo [50] и длинных считываний секвенирования [51]. 
Идти к: 
4. Биоинформатика NGS 

Как обсуждалось выше, платформы для секвенирования становятся более эффективными и производительными, теперь стало возможным полностью секвенировать геном человека в течение одной недели по относительно доступной цене (PromethION обещает доставку генома человека менее чем за 1000 долларов [52]). Следовательно, объем генерируемых данных требует навыков вычислений и биоинформатики для управления, анализа и интерпретации огромного количества данных NGS. Таким образом, в области NGS (био) информатики происходит значительное развитие, которое могло произойти только в значительной степени из-за увеличения вычислительных мощностей (оборудования), а также алгоритмов и приложений (программного обеспечения) для выполнения всех необходимых шагов: от необработанных данных обработка для более детального анализа данных и интерпретации вариантов в клиническом контексте. Как правило, биоинформатика NGS подразделяется на первичный, вторичный и третичный анализ (рис. 2). Общая цель каждого анализа в основном одна и та же независимо от платформы NGS, однако каждая платформа имеет свои особенности и особенности. Для простоты мы сосредоточились на двух основных коммерческих платформах 2-го поколения: Illumina и Ion Torrent. 
Внешний файл, содержащий изображение, иллюстрацию и т. Д. Имя объекта: jcm-09-00132-g002.jpg 
Открыть в отдельном окне 
фигура 2 

Обзор рабочего процесса биоинформатики секвенирования следующего поколения (NGS). Биоинформатика NGS подразделяется на первичный (синий), вторичный (оранжевый) и третичный (зеленый) анализ. Первичный анализ данных состоит из обнаружения и анализа необработанных данных. Затем при вторичном анализе считывания сравниваются с эталонным геномом человека (или собранным de novo) и выполняется вызов. Последний шаг - это третичный анализ, который включает аннотацию вариантов, фильтрацию вариантов, приоритизацию, визуализацию данных и создание отчетов. CNV - вариация количества копий; ROH - серии гомозиготности, VCF - вариантный формат вызова. 
4.1. Первичный анализ 

Первичный анализ данных состоит из обнаружения и анализа необработанных данных (анализ сигналов), нацеленного на создание разборчивых считываний секвенирования (вызов базы) и оценку качества базы. Типичными выходными данными этого первичного анализа являются файл FASTQ (Illumina) или файл несопоставленной двоичной карты выравнивания (uBAM) (Ion Torrent). 

4.1.1. Ион Торрент 

В платформе Ion Torrent эта задача в основном выполняется в программном обеспечении Ion Torrent Suite [53]. Как упоминалось ранее, он начинается с обработки сигнала, в которой сигнал включения нуклеотидов обнаруживается датчиком в нижней части ячейки чипа, преобразуется в напряжение и передается от секвенсора на сервер в виде необработанных данных напряжения, называемых файлом DAT. Для каждого потока нуклеотидов создается один файл сбора данных, который содержит измерение необработанного сигнала в каждой лунке чипа для данного потока нуклеотидов. Во время конвейера анализа эти измерения необработанного сигнала преобразуются в меры включения, называемые файлом WELLS (рисунок 3). Базовый вызов - это заключительный этап первичного анализа, который выполняется модулем основного вызывающего абонента. Его цель - определить наиболее вероятную базовую последовательность, наиболее подходящую для сигнала включения, хранящегося в файле WELLS. Математические модели, лежащие в основе этого базового вызова, сложны (включая эвристический и генеративный подходы) и состоят из трех подэтапов, а именно: нормализация на основе ключевой последовательности, итеративная / адаптивная нормализация и фазовая коррекция [53]. 
Внешний файл, содержащий изображение, иллюстрацию и т. Д. Имя объекта - jcm-09-00132-g003.jpg 
Открыть в отдельном окне 
Рисунок 3 

Схематическое изображение рабочего процесса первичного анализа в Ion Torrent. Вкратце, сигнал, испускаемый при включении нуклеотидов, проверяется датчиком, который преобразует необработанные данные о напряжении в файл DAT. Этот файл служит входными данными для сервера, который преобразуется в файл WELLS. Этот последний файл используется в качестве входных данных для модуля Ion Torrent Basecaller, который дает окончательный файл BAM, готовый для вторичного анализа. 

Такая процедура требуется для устранения некоторых ошибок, возникающих во время процесса SBS, а именно фазирования или спада сигнала (DR, который представляет собой затухание сигнала по каждому потоку, это связано с тем, что некоторые из клонов шаблона, прикрепленных к шарику, терминируются, и нет большее включение нуклеотидов). Эти ошибки возникают довольно часто, и поэтому в качестве начального шага Ion Torrent выполняет алгоритмы фазовой коррекции и нормализации затухания сигнала. Три параметра, которые участвуют в генерации сигнала, - это перенос (CF, то есть неправильное связывание нуклеотида), неполное удлинение (IE, например, протекающий нуклеотид не прикрепился к правильному положению матрицы) и падение (ДР). CF и IE регулируют скорость нефазового накопления полимеразы, в то время как DR измеряет скорость потери ДНК-полимеразы во время цикла секвенирования. Область чипа разделена на определенные области, и каждая лунка дополнительно разделена на две группы (с нечетными и четными номерами), каждая из которых получает свой собственный независимый набор оценок. Затем выбираются некоторые тренировочные скважины, которые используются для поиска оптимальных CF, IE и DR с использованием алгоритма оптимизации Нелдера – Мида [53]. Это модель, в которой используется форма треугольника или симплекс, например обобщенный треугольник в N измерениях, для поиска оптимального решения в многомерном пространстве [54]. 

Измерения CF, IE и DR, а также нормализованные сигналы используются Solver, который следует алгоритму ветвей и границ. Алгоритм ветвей и границ состоит из систематического перечисления всех частичных последовательностей, образующих корневое дерево, при этом набор возможных решений размещается как ветви этого дерева. Алгоритм расширяет каждую ветвь, сравнивая ее с оптимальным решением (теоретическим), и продолжает и продолжает, пока не найдет решение, более близкое к оптимальному [55]. Перед первым запуском решателя необходимо выполнить нормализацию ключа. Ключевая нормализация основана на предположении, что сигнал, испускаемый во время включения нуклеотида, теоретически равен 1, а для невключения испускаемый сигнал равен 0. Таким образом, ключевая нормализация масштабирует измерения с постоянным коэффициентом, который выбирается, чтобы принести сигнал известных ожидаемых 1-мер, полученный путем секвенирования через ключ как можно ближе к единице. После того, как решающая программа выбрала базовую последовательность, предсказанный сигнал используется для создания адаптивной нормализации, которая сравнивает теоретический сигнал с реальным, чтобы впоследствии сгенерировать улучшенный нормализованный сигнал с уменьшенной частотой ошибок [53]. Это входит в цикл последующих итераций (это означает многократное применение функции, при этом выход одной из них является входом следующей) с решателем в течение нескольких раундов, позволяя нормализованному сигналу и предсказанному сигналу сходиться к лучшему решению. . Этот алгоритм заканчивается списком многообещающих частичных последовательностей, на основе которых оценивается наиболее вероятная базовая последовательность в скважине. 

4.1.2. Иллюмина 

Что касается платформы Illumina, принцип обнаружения сигнала основан на флуоресценции. Следовательно, вызов оснований, по-видимому, намного проще, он производится непосредственно из измерений интенсивности флуоресцентного сигнала, полученного в результате включения нуклеотидов во время каждого цикла. Illumina утверждает, что их технология SBS обеспечивает самый высокий процент чтения без ошибок. Последние версии их химии были повторно оптимизированы для обеспечения точного определения оснований в сложных областях генома, таких как GC богатые, гомополимеры и повторяющиеся по природе [56]. Кроме того, дНТФ были химически модифицированы, чтобы содержать обратимую блокирующую группу, которая действует как временный терминатор для полимеризации ДНК. После каждого включения dNTP изображение обрабатывается для идентификации соответствующего основания, а затем ферментативно отщепляется, чтобы позволить включение следующего основания [56]. Однако одна проточная кювета часто содержит миллиарды кластеров ДНК, плотно и беспорядочно упакованных в очень небольшую область. Такая физическая близость может привести к перекрестным помехам между соседними кластерами ДНК. Поскольку флуорофоры, прикрепленные к каждой базе, производят световое излучение, между нуклеотидными сигналами может быть некоторая степень интерференции, которая может перекрываться с оптимальным излучением флуорофоров окружающих кластеров. Таким образом, хотя базовый вызов проще, чем в Ion Torrent, этап обработки изображения довольно сложен. Общий процесс требует совмещения каждого изображения с шаблоном положения кластера на проточной кювете, извлечения изображения для присвоения значения интенсивности для каждого кластера ДНК с последующей коррекцией интенсивности. Помимо этой коррекции перекрестных помех, во время процесса секвенирования возникает еще один проблемный аспект, влияющий на процесс вызова оснований, такой как фазирование (сбои во включении нуклеотидов), затухание (или затухание сигнала) и накопление Т (флуорофоры тимина не всегда эффективно удаляются после каждой итерации. , вызывая накопление сигнала во время цикла секвенирования). В течение многих циклов эти ошибки будут накапливаться и уменьшать общее отношение сигнал / шум на отдельный кластер, вызывая снижение качества к концу считывания [27]. Некоторыми из первоначальных абонентов платформы Illumina были Alta-Cyclic [57] и Bustard [58]. В настоящее время существует множество других вызывающих базовые данные, которые отличаются статистическими и вычислительными методологиями, используемыми для вывода правильной базы [59,60,61]. Несмотря на эту изменчивость (рисунок 4), наиболее широко используемым основным вызывающим абонентом является Bustard, и несколько алгоритмов базового вызова были созданы с использованием Bustard в качестве отправной точки. В целом алгоритм Bustard основан на преобразовании сигналов флуоресценции в фактические данные последовательности. Берутся интенсивности четырех каналов для каждого кластера в каждом цикле, что позволяет определять концентрацию каждого основания. Алгоритм Bustard основан на параметрической модели и применяет алгоритм Маркова для определения вероятности моделирования матрицы перехода для фазирования (новая база не синтезируется), предварительной фазы (синтезированы две новые базы) и нормального включения. Алгоритм Bustard предполагает постоянную матрицу перекрестных помех для данного цикла секвенирования и что фазирование одинаково влияет на все нуклеотиды [58]. С целью повышения производительности и уменьшения количества ошибок было разработано несколько базовых вызывающих программ, однако нет никаких доказательств того, что данный базовый вызывающий абонент лучше, чем другой [59,60]. Сравнение производительности различных базовых вызывающих, а именно в отношении частоты выравнивания, частоты ошибок и времени работы, показывает, что AYB представляет самую низкую частоту ошибок, BlindCall - самую быструю, а BayesCal - лучшую скорость выравнивания. BayesCall, freeIbis, Ibis, naiveBayesCall, Softy FB и Softy SOVA не показали значительных различий между собой, но все они показали улучшение частоты ошибок по сравнению со стандартной Bustard [60]. Недавно был разработан базовый вызывающий 3Dec для платформ секвенирования Illumina, который, как утверждается, уменьшает ошибки базового вызова на 44–69% по сравнению с предыдущими [61]. 
Внешний файл, содержащий изображение, иллюстрацию и т. Д. Имя объекта: jcm-09-00132-g004.jpg 
Открыть в отдельном окне 
Рисунок 4 

Краткое изложение некоторых широко используемых программ базовых абонентов, доступных для платформы Illumina. Программное обеспечение сгруппировано в соответствии с входным файлом: текстовый формат INT (промежуточный исполняемый код) для старых инструментов и CIF (файлы интенсивности кластера) для самых последних платформ. 

4.1.3. Контроль качества: фильтрация и обрезка чтения 

Поскольку ошибки возникают как в секвенировании Ion Torrent, так и в Illumina, они выражаются в показателях качества базового вызова, основанном на оценке Phred, логарифмической вероятности ошибки. Таким образом, оценка Phred, равная 10 (Q10), относится к основанию с вероятностью 1 из 10 быть неправильным или с точностью 90,0%, а как для Q30 означает вероятность 1 из 1000 неверного основания или точность 99,9% [62] . Файлы Fastq важны для первого шага контроля качества, так как содержат все считанные необработанные последовательности операций чтения, имена файлов и значения качества, причем более высокие числа указывают на более высокое качество [63]. Качество исходной последовательности имеет решающее значение для общего успеха анализа NGS, поэтому для оценки качества исходных данных было разработано несколько биоинформатических инструментов, таких как инструментарий NGS QC [64], QC-Chain [65] и FastQC [66]. ]. FastQC - один из самых популярных. На выходе FastQC дает отчет, содержащий хорошо структурированную и графически иллюстрированную информацию о различных аспектах качества чтения. Если считывание последовательностей имеет достаточное качество, последовательности готовы к выравниванию / картированию относительно эталонного генома. Оценка Phred также полезна для фильтрации и обрезки последовательностей. Дополнительная обрезка выполняется в конце каждого чтения для удаления последовательностей адаптеров. В целом шаг обрезки, хотя и уменьшает общее количество и длину считываний, повышает качество до приемлемого уровня. Для выполнения обрезки было разработано несколько инструментов, а именно с данными Illumina, например BTrim [67], IeeHom [68], AdapterRemoval [69] и Trimmonatic [70]. Выбор инструмента во многом зависит от набора данных, последующего анализа и используемых параметров [71]. В Ion Torrent упорядочивание и управление данными обрабатываются в программном обеспечении Torrent Suite, в котором в качестве веб-интерфейса используется Torrent Browser. Для дальнейшего анализа последовательностей требуется процесс демультиплексирования, который разделяет считанные данные секвенирования в отдельные файлы в соответствии со штрих-кодами, используемыми для каждого образца [72]. Большинство протоколов демультиплексирования предназначены для производителей платформ NGS. 

Демультиплексирование - это следующий этап обрезки адаптера, функция которого заключается в удалении оставшихся последовательностей адаптеров библиотеки с конца чтения, в большинстве случаев с 3-го конца, но может зависеть от подготовки библиотеки. Этот шаг необходим, поскольку остаточные последовательности адаптеров при чтении могут мешать отображению и сборке. Fabbro и соавторы показали, что обрезка важна именно в процедурах RNA-Seq, идентификации SNP и сборки генома, повышая качество и надежность анализа, с дальнейшим выигрышем с точки зрения времени выполнения и необходимых вычислительных ресурсов [71]. Для выполнения обрезки используются несколько инструментов, а именно данные Illumina. Опять же, лучшего инструмента не существует, вместо этого выбор зависит от последующего анализа и выбранных пользователем компромиссов, зависящих от параметров [71]. В Ion Torrent это также делается в программном обеспечении Torrent SuiteTM. 
4.2. Вторичный анализ 

Следующим шагом конвейера анализа данных NGS является вторичный анализ, который включает выравнивание считываний относительно эталонного генома человека (обычно hg19 или hg38) и вызов вариантов. Для сопоставления секвенирования чтения можно использовать две разные альтернативы: выравнивание по чтению, то есть выравнивание секвенированных фрагментов относительно эталонного генома, или сборка de novo, которая включает сборку генома с нуля без помощи внешних данных. Выбор между одним или другим подходом может просто зависеть от наличия или отсутствия эталонного генома [73]. Тем не менее, для большинства приложений NGS, а именно в клинической генетике, первым выбором является сопоставление с эталонной последовательностью. Что касается сборки de novo, она по-прежнему в основном ограничивается более конкретными проектами, особенно нацеленными на исправление неточностей в эталонном геноме [74] и улучшение идентификации SV и других сложных реаранжировок [36]. 

Выравнивание последовательностей - классическая проблема, которую решает биоинформатика. Считывания секвенирования с большинства платформ NGS короткие, поэтому для секвенирования генома генерируются миллиарды фрагментов ДНК / РНК, которые необходимо собрать, как головоломку. Это представляет собой серьезную вычислительную проблему, особенно когда речь идет о существовании операций чтения, полученных из повторяющихся элементов, которые в экстремальных условиях алгоритм должен выбрать, из какой повторяющейся копии принадлежит чтение. В таком контексте невозможно совершать вызовы с высокой степенью достоверности, решение должно приниматься либо пользователем, либо программным обеспечением с помощью эвристического подхода. Ошибки выравнивания последовательностей могут возникать по нескольким причинам. Во-первых, ошибки в секвенировании (вызванные такими процессами, как затухание и спад сигнала, как обсуждалось ранее), а также расхождения между секвенированными данными и эталонным геномом также вызывают проблемы несовпадения. Еще одна серьезная трудность - установить порог между реальным отклонением и несоответствием. Наиболее распространенным форматом файла ввода данных для сборки является FASTQ [66]. В качестве выходных данных типичными файлами являются формат двоичного выравнивания / карты (BAM) и выравнивания / сопоставления последовательностей (SAM) из различных платформ секвенирования и считывающих выравнивателей. Оба включают в основном одну и ту же информацию, а именно последовательность считывания, базовые оценки качества, расположение выравниваний, различия относительно эталонной последовательности и оценки качества картирования (MAPQ). Основное различие между ними состоит в том, что формат SAM представляет собой текстовый файл, созданный для упрощения обработки с помощью простых инструментов, в то время как формат BAM предоставляет двоичные версии тех же данных [75]. Выравнивания можно просматривать с помощью удобного и свободно доступного программного обеспечения, такого как Interactive Genome Viewer (IGV) [76] или Genome Browse (http://goldenhelix.com/products/GenomeBrowse/index.html). 

4.2.1. Выравнивание последовательности 

Когда известен эталонный геном, предпочтительный метод сборки - это выравнивание по эталонному геному. Алгоритм сопоставления будет пытаться найти место в эталонной последовательности, которое соответствует считыванию, допуская определенное количество несовпадений, чтобы обеспечить обнаружение изменений подпоследовательности. Было разработано более 60 инструментов для картирования генома, и по мере обновления платформ NGS будет появляться все больше и больше инструментов, которые постоянно развиваются (более подробно см. [77,78]). Среди часто используемых методов для выполнения выравнивания коротких чтений мы выделили выравниватели Барроуза – Уиллера (BWA) и Bowtie, которые в основном используются для Illumina; в то время как для Ion Torrent рекомендуется программа согласования Torrent Mapping Alignment Program (TMAP), поскольку она была специально оптимизирована для этой платформы. 

BWA использует алгоритм преобразования Барроуза-Уиллера (алгоритм преобразования данных, который реструктурирует данные для большей сжимаемости), первоначально разработанный для подготовки данных для таких методов сжатия, как bzip2, это быстрый и эффективный механизм выравнивания, очень хорошо работающий как для коротких, так и для длинных чтений. [79]. Bowtie (теперь Bowtie 2) имеет преимущество в том, что он быстрее, чем BWA для некоторых типов выравнивания [80], но это может ухудшить качество, а именно снижение чувствительности и точности. Bowtie может не согласовать некоторые чтения с действительными сопоставлениями при настройке на максимальную скорость. Обычно он применяется для выравнивания считываний, полученных в экспериментах по секвенированию РНК. Руффало и его коллеги разработали пакет моделирования и оценки Seal для моделирования прогонов с целью сравнения наиболее широко используемых инструментов для картирования, таких как Bowtie, BWA, mr- и mrsFAST, Novoalign, SHRiMP и SOAPv2 [81]. В исследовании сравнивали различные параметры, включая ошибку секвенирования, отступы и покрытие, и пришли к выводу, что идеального инструмента не существует. Каждый из них имеет различные особенности и характеристики, которые зависят от выбора пользователя, например, что имеет приоритет, точность покрытия, несмотря на то, что все проанализированные инструменты не зависят от конкретной платформы [81]. Специальный инструмент ионного торрента, TMAP, состоит из двух этапов: (i) начальное картирование (с использованием алгоритмов Смита – Уотермана или Нидлмана – Вунша), позволяющее приблизительно выровнять считанные данные относительно эталонного генома, и (2) уточнение выравнивания, так как конкретные положения считываемого переставлены в соответствующую позицию в ссылке. Это уточнение выравнивания предназначено для компенсации специфических систематических ошибок процесса секвенирования Ion Torrent (таких как выравнивание гомополимеров и ошибки фазирования с низкими показателями indel) [82]. 

De novo Assembly 

Сборка de novo позволяет обойти систематическую ошибку эталонного генома, ограничения неточностей в эталонном геноме, будучи наиболее эффективной для идентификации SV и сложных реаранжировок, избегая потери новых последовательностей. Большинство алгоритмов сборки de novo основаны на стратегии перекрытия-компоновки, при которой идентифицируются равные области в геноме и затем перекрываются фрагментированными секвенированными концами. Эта стратегия имеет ограничение, заключающееся в предрасположенности к неправильному выравниванию, особенно с короткими длинами считывания, в основном из-за того, что регионы с высокой повторяемостью затрудняют определение того, к какому региону генома они принадлежат. Сборка de novo предпочтительна, когда доступны более длинные чтения. Дальнейшая сборка de novo выполняется намного медленнее и требует больше вычислительных ресурсов (таких как память) по сравнению с сопоставлением сборок [73]. 

Как уже упоминалось, ассемблеры de novo в основном основаны на теории графов и могут быть разделены на три основные группы (Рисунок S1): (i) Согласование схемы перекрытия (OLC), (ii) граф де Брейна (DBG, также известный как Eurelian) методы, использующие некоторую форму графа К-мера и (iii) алгоритмы жадного графа, которые могут использовать либо OLC, либо DBG [78]. 

Вкратце, жадный алгоритм начинается с добавления чтения к другому аналогичному. Этот процесс повторяется до тех пор, пока не будут достигнуты все варианты сборки для этого фрагмента, и повторяется для всех фрагментов. Каждая операция использует перекрытие с наивысшей оценкой для выполнения следующего соединения. Чтобы сделать оценку, алгоритм измеряет, например, количество совпадающих баз в перекрытии. Этот алгоритм подходит для небольшого количества чтений и небольших геномов. Напротив, метод OLC был оптимизирован для длинных чтений с низким охватом [78,83]. OLC начинает с определения перекрытий между парами чтений и строит график отношений между этими чтениями, что требует больших вычислительных затрат, особенно при большом количестве чтений. Как только граф построен, требуется гамильтонов путь (путь в неориентированном / ориентированном графе, который посещает каждую вершину ровно один раз), который порождает последовательности контигов. Чтобы завершить процесс, требуется дальнейшая вычислительная и ручная проверка [83,84]. Ассемблер Eurelian (или метод DBG) особенно подходит для представления отношения перекрытия при коротком чтении. Здесь также строится граф, однако узлы графа представляют k-мер, а ребра представляют собой смежные k-меры, которые перекрываются на k-1 оснований. Следовательно, размер графа определяется размером генома и содержанием повторов в секвенированном образце и, в принципе, на него не влияет высокая избыточность покрытия глубокого чтения [85]. 

Благодаря технологиям секвенирования и картирования на больших расстояниях постоянно создаются секвенсоры третьего поколения и новые инструменты биоинформатики, которые используют уникальные функции и преодолевают ограничения этих новых платформ секвенирования и картирования, такие как высокий уровень ошибок, в котором преобладают ложные вставки или удаления. и разреженное секвенирование, а не настоящие длинные чтения. Некоторые из этих методов были подробно рассмотрены в другом месте [86]. 

4.2.2. Пост-согласовательная обработка 

Перед вызовом варианта рекомендуется обработка после выравнивания. Его цель - повысить точность вызова вариантов и качество последующего процесса за счет уменьшения артефактов базового вызова и выравнивания [87]. В платформе Ion Torrent это включено в программное обеспечение TMAP, тогда как в Illumina требуются другие инструменты. В общих чертах, он состоит из фильтрации (удаления) повторяющихся считываний, интенсивной локальной перестройки (в основном около INDEL) и повторной калибровки базовой оценки качества [88] (рис. 5). 
Внешний файл, содержащий изображение, иллюстрацию и т. Д. Имя объекта - jcm-09-00132-g005.jpg 
Открыть в отдельном окне 
Рисунок 5 

Схематическое изображение основных этапов процесса пост-выравнивания. 

SAMtools [75], Genome Analysis Toolkit (GATK) [89] и Picard (http://broadinstitute.github.io/picard/) - некоторые из биоинформатических инструментов, используемых для выполнения этой обработки после выравнивания. Поскольку алгоритмы вызова вариантов предполагают, что в случае библиотек, основанных на фрагментации, все чтения независимы, удаление дубликатов ПЦР и неуникальных выравниваний (т. Е. Считываний с более чем одним оптимальным выравниванием) имеет решающее значение. Этот шаг можно выполнить с помощью инструментов Picard (например, MarkDuplicates). Если не удалить данный фрагмент, он будет считаться другим считыванием, увеличивая количество неправильных вызовов вариантов и приводя к неправильному охвату и / или оценке генотипа [88]. Чтения, охватывающие INDEL, требуют дальнейшей обработки. Учитывая тот факт, что каждое считывание независимо выравнивается по эталонному геному, когда INDEL является частью считывания, существует более высокое изменение несоответствия выравнивания. Инструмент реорганизации сначала определяет подозрительные интервалы, требующие перестройки из-за наличия INDEL, затем реорганизатор пробегает эти интервалы, комбинируя обрывки свидетельств, чтобы сгенерировать согласованный балл, подтверждающий наличие INDEL [88]. Для выполнения этого шага можно использовать IndelRealigner из пакета GATK. Как уже упоминалось, достоверность базового вызова определяется показателем качества по шкале Phred, который генерируется устройством секвенсора и представляет собой исходный показатель качества. Однако на этот показатель могут влиять несколько факторов, а именно платформа секвенирования и состав последовательности, и не отражать частоту ошибок при вызове базы [90]. Следовательно, необходимо повторно откалибровать этот счет, чтобы повысить точность вызова вариантов. BaseRecalibrator из пакета GATK - один из наиболее часто используемых инструментов. 

4.2.3. Вариант вызова 

Этап вызова варианта имеет основную цель - идентифицировать варианты с помощью постобработанного файла BAM. Для вызова вариантов доступно несколько инструментов, некоторые идентифицируют варианты на основе количества вызовов базы с высокой степенью достоверности, которые не согласуются с интересующей референсной позицией генома. В других случаях используются статистические методы байесовского анализа, вероятностного обучения или машинного обучения, в которых используются факторные параметры, такие как базовые оценки и показатели качества сопоставления, для выявления различий вариантов. Алгоритмы машинного обучения претерпели значительные изменения в последние годы и будут иметь решающее значение, чтобы помочь ученым и клиницистам обрабатывать большие объемы данных и решать сложные биологические задачи [91,92]. 

SAMtools, GATK и Freebayes относятся к последней группе и являются одними из наиболее широко используемых инструментов для работы с данными Illumina [93]. У Ion Torrent есть собственный вариант вызывающего абонента, известный как Torrent Variant Caller (TVC). Выполняясь как плагин на сервере Ion Torrent, TVC вызывает однонуклеотидные полиморфизмы (SNV), многонуклеотидные варианты (MNV), INDELS в образце по ссылке или в целевом подмножестве этой ссылки. Можно настроить несколько параметров, но часто можно использовать заранее определенные конфигурации (строгость зародышевой линии или соматическая, высокая или низкая) в зависимости от типа проводимого эксперимента. 

Большинство этих инструментов используют формат SAM / BAM в качестве входных данных и генерируют файл формата вызова вариантов (VCF) в качестве выходных данных [94]. Формат VCF - это файл стандартного формата, в настоящее время версии 4.2, разработанный крупными проектами секвенирования, такими как проект 1000 геномов. VCF - это, по сути, текстовый файл, содержащий строки метаинформации, строку заголовка, за которой следуют строки данных, каждая из которых содержит информацию о хромосомном положении, справочную базу, идентифицированную альтернативную базу или базы. Формат также содержит информацию о генотипе образцов для каждой позиции [95]. VCFtools предоставляют возможность легко манипулировать файлами VCF, например, объединять несколько файлов или извлекать SNP из определенных регионов [94]. 

Вызов вариантов конструкции 

В геноме человека могут встречаться генетические вариации от SNV и INDELS до более сложных (субмикроскопических) SV [96]. Эти SV включают как большие вставки / дупликации и делеции (также известные как варианты числа копий, CNV), так и большие инверсии и могут иметь большое влияние на здоровье [97]. Секвенсоры с более длинным считыванием обещают выявить большие структурные вариации и причинные мутации в нераскрытых генетических заболеваниях [36,98,99]. Включение вызова такого SV увеличит диагностическую ценность этих подходов NGS, преодолевая некоторые ограничения, присутствующие в других методах, и с потенциалом в конечном итоге их заменить. Отражая эту растущую тенденцию, было разработано несколько биоинформатических инструментов для обнаружения CNV из данных NGS. В настоящее время для обнаружения CNV из данных NGS можно использовать пять подходов в соответствии с типом используемых алгоритмов / стратегий (рис. 6): парное картирование, разделенное чтение, глубина чтения, сборка генома de novo и комбинаторный подход [100,101]. 
Внешний файл, содержащий изображение, иллюстрацию и т. Д. Имя объекта - jcm-09-00132-g006.jpg 
Открыть в отдельном окне 
Рисунок 6 

Краткое изложение основных методов вызова структурных вариантов (SV) и вариации числа копий (CNV) из данных секвенирования следующего поколения (NGS). 

Алгоритм сопоставления парных концов сравнивает средний размер вставки между существующими секвенированными парами чтения с ожидаемым размером на основе эталонного генома. Карты несогласованного чтения могут указывать на наличие удаления или вставки. Методы картирования парных концов могут также эффективно идентифицировать вставки мобильных элементов, вставки (меньше, чем средний размер вставки инверсий библиотеки генома) и тандемные дупликации [100]. Ограничением картирования парных концов является неспособность обнаружить CNVs в регионах низкой сложности с сегментарными дупликациями [102]. BreakDancer [103] и PEMer [102] - два примера инструментов, использующих стратегию сопоставления парных концов. Что касается методов раздельного чтения, он основан на концепции, согласно которой из-за структурного варианта только одно из считываний пары правильно выровнено с эталонным геномом, в то время как другое не может отображаться или выравнивается только частично [100]. Последние чтения также могут предоставить точные контрольные точки лежащего в основе генетического дефекта на уровне пары оснований. Частично картированные чтения объединяются в несколько фрагментов, которые независимо выравниваются по эталонному геному [100,101]. Этот дальнейший шаг переназначения обеспечивает точную начальную и конечную позиции событий вставки / удаления. Некоторыми примерами инструментов, использующих метод раздельного чтения, являются Pindel [104] и SLOPE [105]. Что касается методологии глубины чтения, она исходит из предположения, что можно установить корреляцию между количеством копий и глубиной охвата чтения. Дизайн исследования, что касается нормализации или сравнения данных, может основываться на отдельных выборках, парных случаях, контрольных выборках или большом наборе данных (генеральной совокупности) выборок. В общих чертах эти алгоритмы представляют собой основные этапы: отображение, нормализацию, оценку количества копий и сегментацию [100]. Во-первых, чтения выравниваются, и охват оценивается по отдельному геному через заранее определенные интервалы. Затем алгоритм вызова CNV должен выполнить нормализацию с точки зрения количества чтений, чтобы компенсировать потенциальные смещения, например, из-за содержимого GC или повторяющихся последовательностей. Нормализация результатов CNV является сложной задачей из-за естественных вариаций CNV. Эта проблема еще более усугубляется при раке, поскольку онкологические заболевания имеют огромный спектр типов опухолей CNV внутри, а также межклеточные вариации [106,107]. Имея нормализованную глубину чтения, можно получить приблизительную оценку количества копий (прирост или убыток) по геномным сегментам. Что касается последнего шага, сегментации, области с аналогичным количеством копий объединяются для обнаружения аномального количества копий [100,108]. По сравнению с методами парного и разделенного чтения подходы глубины чтения имеют преимущество в оценке количества копий CNV и тенденцию к достижению лучших характеристик с большими CNV [100]. 

Выявление CNV в основном основывается на данных полногеномного секвенирования (WGS), поскольку включает некодирующие области, которые, как известно, охватывают значительный процент SV [109]. Секвенирование всего экзома (WES) стало более рентабельной альтернативой WGS, и интерес к обнаружению CNV по данным WES значительно вырос. Однако, поскольку только небольшая часть генома человека секвенирована WES, он не может обнаружить полный спектр CNV. Более низкая однородность WES по сравнению с WGS может снизить его чувствительность к обнаружению CNV. Обычно WES создает большую глубину для целевых регионов по сравнению с WGS. Следовательно, большинство инструментов, разработанных для обнаружения CNV с использованием данных WES, имеют реализованные алгоритмы вызова на основе глубины и требуют в качестве входных данных нескольких выборок или сопоставленных выборок для контроля случая [100]. Ion Torrent также разработал собственный алгоритм, как часть программного обеспечения Ion Reporter, для обнаружения CNV в данных NGS, полученных из библиотек на основе ампликонов [110]. 
4.3. Третичный анализ 

Третий основной этап процесса анализа NGS направлен на решение важной проблемы «осмысления» или интерпретации данных, то есть нахождения в контексте клинической генетики человека фундаментальной связи между вариантными данными и фенотипом, наблюдаемым у пациента. Третичный анализ начинается с аннотации вариантов, которая добавляет дополнительный уровень информации для прогнозирования функционального воздействия всех вариантов, ранее обнаруженных на этапах вызова вариантов. После аннотации вариантов следуют инструменты фильтрации, приоритезации и визуализации данных. Эти аналитические шаги могут выполняться с помощью комбинации широкого спектра программного обеспечения, которое должно постоянно обновляться, чтобы включать в себя последние научные открытия, требующие постоянной поддержки и дальнейших улучшений со стороны разработчиков. 

4.3.1. Вариант аннотации 

Аннотации вариантов - это ключевой начальный шаг для анализа вариантов секвенирования. Как упоминалось ранее, результатом вызова варианта является файл VCF. Каждая строка в таком файле содержит высокоуровневую информацию о варианте, такую ​​как геномное положение, справочные и альтернативные базы, но не данные о его биологических последствиях. Аннотация варианта предлагает такой биологический контекст для всех найденных вариантов. Учитывая огромный объем данных NGS, аннотация данных выполняется автоматически. В настоящее время доступно несколько инструментов, каждый из которых использует разные методологии и базы данных для аннотации вариантов. Большинство инструментов могут выполнять как аннотацию SNV, так и аннотацию INDEL, тогда как аннотации SV или CNV являются более сложными и выполняются не всеми методами [111]. Один из основных шагов в аннотации - предоставить контекст варианта. То есть в каком гене расположен вариант, его положение в гене и влияние вариации (бессмысленность, бессмыслица, синоним, стоп-лосс и т. Д.). Такие инструменты аннотации предлагают дополнительную аннотацию на основе функциональности, интегрируя другие алгоритмы, такие как SIFT [112], PolyPhen-2 [113], CADD [114] и Condel [115], которые вычисляют оценки последствий для каждого варианта на основе различных различных параметров. , например, степень сохранения аминокислотных остатков, гомология последовательностей, эволюционная консервация, структура белка или статистический прогноз, основанный на известных мутациях. Дополнительная аннотация может использоваться в базах данных вариантов заболевания, таких как ClinVar и HGMD, где извлекается информация о его клинической ассоциации. Среди обширного списка инструментов аннотации наиболее широко используются ANNOVAR [116], вариантный предсказатель эффекта (VEP) [117], snpEff [118] и SeattleSeq [119]. ANNOVAR - это инструмент командной строки, который может идентифицировать SNP, INDEL и CNV. Он аннотирует функциональные эффекты вариантов по отношению к генам и другим геномным элементам и сравнивает варианты с существующими базами данных вариаций. ANNOVAR также может оценивать и отфильтровывать подмножества вариантов, которые не сообщаются в общедоступных базах данных, что особенно важно при работе с редкими вариантами, вызывающими менделевские заболевания [120]. Как и ANNOVAR, VEP от Ensembl (EMBL-EBI) может предоставить геномную аннотацию для многих видов. Однако, в отличие от ANNOVAR, который требует установки программного обеспечения и опытных пользователей, VEP имеет удобный интерфейс через специальный веб-браузер генома, хотя он может иметь программный доступ через автономный сценарий Perl или REST API. Поддерживается более широкий диапазон форматов входных файлов, и он может аннотировать SNP, indels, CNV или SV. VEP выполняет поиск в базе данных Ensembl Core и определяет, где в геномной структуре находится вариант, и в зависимости от этого дает прогноз последствий. SnpEff - еще один широко используемый инструмент аннотации, автономный или интегрированный с другими инструментами, обычно используемыми в конвейерах анализа данных, таких как поддержка проектов Galaxy, GATK и GKNO. В отличие от VEP и ANNOVAR, он не аннотирует CNV, но имеет возможность аннотировать некодирующие области. Он может выполнять аннотацию для нескольких вариантов быстрее, чем VEP [118]. 

Вариант аннотации может показаться простым и понятным процессом; однако это может быть очень сложно, учитывая запутанность генетической организации. Теоретически экзонные области генома транскрибируются в РНК, которая, в свою очередь, транслируется в белок. Создание этого одного гена приведет к возникновению только одного транскрипта и, в конечном итоге, одного белка. Однако такая концепция (гипотеза «один ген - один фермент») полностью устарела, поскольку генетическая организация и ее механизм намного сложнее. Благодаря процессу, известному как альтернативный сплайсинг, из одного и того же гена может быть произведено несколько транскриптов и, следовательно, разные белки. Альтернативный сплайсинг является основным механизмом обогащения разнообразия транскриптомов и протеомов [121]. Хотя это необходимо для объяснения генетического разнообразия, при рассмотрении аннотации это серьезная неудача, в зависимости от выбора транскрипта биологическая информация и последствия, касающиеся варианта, могут быть самыми разными. Дополнительная размытость в отношении инструментов аннотации вызвана существованием разнообразия баз данных и наборов данных эталонных геномов, которые не полностью согласованы и частично совпадают по содержанию. Наиболее часто используются Ensembl (http://www.ensembl.org), RefSeq (http://www.ncbi.nlm.nih.gov/RefSeq/) и браузер генома UCSC (http: //genome.ucsc. edu), которые содержат такие справочные наборы данных и дополнительную генетическую информацию для нескольких видов. Эти базы данных также содержат компиляцию различных наборов транскриптов, которые наблюдались для каждого гена и используются для аннотации вариантов. Каждая база данных имеет свои особенности, и поэтому в зависимости от базы данных, используемой для аннотации, результат может быть разным. Например, если для данного локуса один из возможных транскриптов имеет сохраненный интрон, а в других нет, вариант, расположенный в такой области, будет считаться локализованным при кодирующем секвенировании только в одной из изоформ. Чтобы свести к минимуму проблему множественных транскриптов, был разработан проект совместной консенсусной кодирующей последовательности (CCDS). Этот проект направлен на каталогизацию идентичных аннотаций белков в референсных геномах человека и мыши со стабильными идентификаторами и унификацию их представления в различных базах данных [122]. Использование различных инструментов аннотаций также вносит большую вариативность в данные NGS. Например, ANNOVAR по умолчанию использует окно размером 1 Кбайт для определения восходящего и нисходящего регионов [120], тогда как SnpEff и VEP используют 5 Кбайт [117,118]. Это отличает классификацию вариантов, даже если использовалась одна и та же расшифровка. Маккарти с соавторами обнаружили значительные различия в аннотациях VEP и ANNOVAR одного и того же транскрипта [123]. Помимо проблем, связанных с множественными транскриптами и инструментами аннотации, существуют также проблемы с перекрывающимися генами, то есть более чем одним геном в одной геномной позиции. По-прежнему нет полного / окончательного решения для устранения этих ограничений, поэтому результаты вариантной аннотации следует анализировать с учетом проблемы контекста исследования и, если возможно, обращаться к нескольким источникам. 

4.3.2. Вариантная фильтрация, приоритезация и визуализация 

После аннотации файла VCF из WES общее количество вариантов может варьироваться от 30 000 до 50 000. Чтобы разобраться в таком большом количестве вариантов и определить вызывающий заболевание вариант (ы), требуются некоторые стратегии фильтрации. Хотя контроль качества выполнялся на предыдущих этапах, несколько ложноположительных вариантов все еще присутствуют. Следовательно, при запуске третьего уровня анализа NGS настоятельно рекомендуется, основываясь на параметрах качества или предшествующем знании артефактов, уменьшить количество ложноположительных вызовов и ошибок вариантных вызовов. Такие параметры, как общее количество независимых чтений и процент чтений, показывающих вариант, а также длина гомополимера (особенно для Ion Torrent, когда участки длиной более пяти оснований являются подозрительными) являются примерами фильтров, которые можно применить. Пользователь должен определить порог на основе наблюдаемых данных и вопроса исследования, но относительно первого параметра обычно отклоняется менее 10 независимых считываний, поскольку это, вероятно, связано с систематической ошибкой или низким охватом. 

Одним из наиболее часто используемых фильтров NGS является частотный фильтр населения. Частота минорных аллелей (MAF), один из показателей, используемых для фильтрации на основе частоты аллелей, может сортировать варианты по трем группам: редкие варианты (MAF <0,5, обычно выбираются при изучении менделевских заболеваний), варианты с низкой частотой (MAF от 0,5% до 5%) и распространенные варианты (MAF> 5%) [124]. Популяционные базы данных, которые включают данные тысяч людей из нескольких популяций, представляют собой мощный источник вариантной информации о глобальных моделях генетической изменчивости человека. Это также помогает не только лучше идентифицировать аллели болезней, но и важно для понимания происхождения популяции, миграции, взаимоотношений, примесей и изменений в размере популяции, что может быть полезно для понимания характера некоторых болезней [125]. Базы данных о популяциях, такие как проект «1000 геномов» [126], Консорциум агрегации экзомов (ExAC) [127] и База данных агрегирования генома (gnomAD; http://gnomad.broadinstitute.org/), являются наиболее широко используемыми базами данных. Тем не менее, этот фильтр также имеет ограничения и может вызвать ошибочное исключение, которое трудно преодолеть. Например, поскольку носители рецессивных расстройств не проявляют никаких признаков заболевания, частота повреждающих аллелей в базах данных популяционных вариантов может быть выше установленного порога. Собственные базы данных вариантов важны для помощи в фильтрации вариантов, а именно, чтобы помочь понять частоту вариантов в исследуемой популяции и выявить систематические ошибки внутренней системы. 

Многочисленные стандартизированные и общепринятые руководящие принципы для оценки геномных вариаций, полученные с помощью NGS, такие как Американский колледж медицинской генетики и геномики (ACMG) и руководящие принципы Европейского общества генетики человека. Они содержат стандарты и рекомендации по интерпретации геномных вариаций [128,129]. 

В соответствии с узнаваемым образцом наследования рекомендуется выполнять фильтрацию модели на основе семейного наследования. Это особенно полезно, если для изучения доступно более одного пациента из таких семей, поскольку это значительно уменьшит количество вариантов, которые необходимо тщательно проанализировать. Например, для заболеваний с аутосомно-доминантным (АД) типом наследования идеальной ситуацией было бы тестирование как минимум трех пациентов, каждый из другого поколения, и выбор только гетерозиготных вариантов, расположенных в аутосомах. Если родословная указывает на вероятное х-сцепленное заболевание, варианты, расположенные в Х-хромосоме, отбираются, а варианты в других хромосомах в первую очередь не исследуются. Что касается аутосомно-рецессивных (AR) заболеваний с более чем одним пораженным братом или сестрой, было бы важно изучить как можно больше пациентов и выбрать гомозиготные варианты у пациентов, которые были обнаружены в гетерозиготности у обоих родителей, или гены с двумя гетерозиготными вариантами с отличное отцовское происхождение. Для единичных случаев (и случаев, в которых картина болезни неизвестна), тройной анализ может оказаться чрезвычайно полезным для снижения аналитической нагрузки. В таком контексте гетерозиготные варианты, обнаруженные только у пациента и не присутствующие у обоих родителей, будут указывать на происхождение de novo. Даже в несвязанных случаях с очень однородными фенотипами, такими как обычно синдромальные, можно использовать стратегию на основе перекрытия [130], предполагая, что один и тот же ген или даже один и тот же вариант используется всеми пациентами. Дополнительный фильтр, полезный, когда многие варианты сохраняются после применения других, основан на прогнозируемом влиянии вариантов (функциональный фильтр). В некоторых трубопроводах интронные или синонимичные варианты, основанные на предположении, что они могут быть доброкачественными (не связанными с заболеванием). Тем не менее, следует проявлять осторожность, так как многочисленные интронные и очевидные синонимичные варианты были вовлечены в заболевания человека [131,132]. Таким образом, применяется функциональный фильтр, в котором варианты имеют приоритет на основе их геномного местоположения (экзонных или сплайс-сайтов). Дополнительная информация для фильтрации вариантов миссенс включает эволюционную консервацию, прогнозируемое влияние на структуру, функцию или взаимодействия белка. Чтобы включить такую ​​фильтрацию, оценки, генерируемые алгоритмами для оценки бессмысленных вариантов (например, PolyPhen-2, SIFT и CADD), аннотируются в VCF. То же самое относится к вариантам, которые могут повлиять на сращивание, поскольку алгоритмы прогнозирования включаются в аннотацию VCF, например, Human Splice finder [133] в VarAFT [134] (еще несколько примеров в таблице 1). 
Таблица 1 

Перечислите примеры широко используемых инструментов для выполнения функционального фильтра NGS. 
Программное обеспечение Краткое описание Ref. 
Филоп 
Филогенетические p-значения На основе модели нейтральной эволюции анализируются закономерности сохранения (положительные оценки) / ускорения (отрицательные оценки) для различных классов аннотаций и представляющих интерес клад. [146] 
ПРОСЕЯТЬ 
Сортировка нетолерантных от толерантных предсказывает на основе гомологии последовательностей, если замена AA повлияет на функцию белка и потенциально изменит фенотип. Оценка менее 0,05 указывает на вредоносный вариант. [112] 
Полиморфизм Полифен-2 
Phenotyping v2 Прогнозирует функциональное влияние замены AA на основе его индивидуальных особенностей с использованием наивного байесовского классификатора. Включает два инструмента HumDiv (предназначенный для применения в сложных фенотипах) и HumVar (разработанный для диагностики менделевских заболеваний). Более высокие баллы (> 0,85) позволяют более уверенно прогнозировать разрушительные варианты. [113] 
CADD 
Комбинированное истощение, зависящее от аннотаций. Объединяет различные аннотации генома и оценивает все человеческие SNV и Indel. Он определяет приоритетность функциональных, вредных и причинных вариантов заболевания в соответствии с функциональными категориями, величиной эффекта и генетической архитектурой. Баллы выше 10 должны использоваться в качестве порогового значения для выявления патогенных вариантов. [114] 
MutationTaster Анализирует эволюционную консервацию, изменения сайтов сплайсинга, потерю свойств белка и изменения, которые могут повлиять на количество мРНК. Варианты классифицируются как полиморфизм или вызывающие заболевания [147] 
Human Splice Finder Предсказывайте эффекты мутаций на сигналы сплайсинга или идентифицируйте мотивы сплайсинга в любой человеческой последовательности. [133] 
nsSNPAnalyzer извлекает структурную и эволюционную информацию из запроса nsSNP и использует метод машинного обучения (случайный лес) для прогнозирования его фенотипического эффекта. Классифицирует вариант как нейтральный и болезненный. [148] 
TopoSNP 
Топографическое картирование SNP. Анализ SNP на основе его геометрического местоположения и информации о сохранении обеспечивает интерактивную визуализацию заболеваний и не заболеваний, связанных с каждым SNP. [149] 
Кондель 
Consensus Deleteriousness Condel объединяет результаты различных методов для прогнозирования воздействия nsSNP на функцию белка. Алгоритм, основанный на средневзвешенном значении нормализованных оценок, классифицирует варианты как нейтральные или вредоносные. [115] 
ANNOVAR * 
Аннотировать вариации Аннотирует варианты на основе нескольких параметров, таких как определение того, влияют ли SNP или CNV на белок (на основе генов), идентификация вариантов в определенных геномных областях за пределами кодирующих белки областей (на основе области) и идентификация известных вариантов, задокументированных в общедоступная и лицензионная база данных (на основе фильтров) [116] 
ВЭП * 
Предиктор эффекта варианта Определяет влияние нескольких вариантов (SNP, вставок, делеций, CNV или структурных вариантов) на гены, транскрипты и белковые последовательности, а также на регуляторные области. [117] 
snpEff * Аннотация и классификация SNV, основанная на их влиянии на аннотированные гены, таких как синонимы / nsSNP, прирост или потеря стартовых или стоп-кодонов, их геномное расположение, среди прочего. Считается структурным инструментом для аннотации. [118] 
SeattleSeq * Обеспечивает аннотацию SNV и небольшие вставки, предоставляя каждому dbSNP идентификаторы rs, имена генов и номера доступа, функции вариаций, положения белков и изменения AA, оценки сохранения, частоты HapMap, прогнозы PolyPhen и клиническую ассоциацию. [119] 
Открыть в отдельном окне 

AA - аминокислота; SNV - однонуклеотидный вариант, Indel - малые инсерционные / делеционные варианты, SNP - однонуклеотидный полиморфизм, nsSNP - несинонимичный SNP; CNV - вариация количества копий; * эти инструменты, хотя и могут фильтровать варианты, в первую очередь отвечают за аннотацию вариантов. 

Хотя функциональная аннотация добавляет важный уровень информации для фильтрации, фундаментальный вопрос, на который нужно ответить, особенно в контексте открытия генов, заключается в том, действительно ли конкретный вариант или мутированный ген вызывает заболевание [135, 136]. Для решения этого сложного вопроса разрабатывается новое поколение инструментов, которые вместо простого исключения информации выполняют ранжирование вариантов, тем самым позволяя приоритезацию. Предлагались разные подходы. Например, PHIVE исследует сходство между фенотипом заболеваний человека и фенотипами, полученными в результате нокаут-экспериментов на модельных животных [137]. Другие алгоритмы пытаются решить эту проблему совершенно другим способом, путем вычисления оценки вредоносности (также известной как оценка нагрузки) для каждого гена на основе того, насколько гены нетерпимы к нормальной вариации, и с использованием данных из баз данных популяционных вариаций [138] . Было высказано предположение, что гены болезней человека гораздо более нетерпимы к вариантам, чем гены, не связанные с заболеванием [139, 140]. Онтология фенотипа человека (HPO) обеспечивает иерархическую сортировку по названиям болезней и клиническим признакам (симптомам) для описания заболеваний. Основываясь на этих описаниях, HPO может также обеспечить связь между симптомами и известными генами болезни. Некоторые инструменты пытаются использовать эти описания фенотипов для создания рейтинга потенциальных кандидатов при расстановке приоритетов вариантов. Например, некоторые попытки упростить анализ в клиническом контексте, такие как фенотипическая интерпретация экзомов [141], которая сообщает только о генах, ранее связанных с генетическими заболеваниями. В то время как другие также могут быть использованы для идентификации новых генов, таких как Phevor [142], которые используют данные, собранные в других родственных онтологиях, например, онтологии генов (GO), чтобы предложить новые ассоциации ген-болезнь. Основная цель этих инструментов состоит в том, чтобы получить несколько вариантов для дальнейшей проверки с помощью молекулярных методов [143,144]. Недавно было разработано несколько коммерческих программ для помощи в интерпретации и приоритизации вариантов в диагностическом контексте, которые просты в использовании, интуитивно понятны и могут использоваться клиницистами, генетиками и исследователями, например VarSeq / VSClinical (Golden Helix), Ingenuity Вариантный анализ (Qiagen), программное обеспечение Alamut® (интерактивное биопрограммное обеспечение) и VarElect [145]. Помимо этих инструментов, которые помогают в интерпретации и анализе вариантов, в настоящее время врачи имеют в своем распоряжении несколько компаний, занимающихся медицинской генетикой, таких как Invitae (https://www.invitae.com/en/) и CENTOGENE (https: //www.centogen. com /), которые предоставляют врачам точный медицинский диагноз. 
Идти к: 
5. Подводные камни NGS 

Прошло семнадцать лет с момента появления первой коммерчески доступной платформы NGS, 454 GS FLX от Life Sciences. С тех пор область «геномики» значительно расширила наши знания о структурной и функциональной геномике и генетике, лежащей в основе многих заболеваний. Кроме того, он позволяет создавать концепции «омики» (транскриптомический, геномный, метаболомный и т. Д.), Которые обеспечивают новое понимание знаний всех живых существ, чтобы узнать, как разные организмы используют генетику и молекулярную биологию для выживания и воспроизводства. в здоровых и болезненных ситуациях, чтобы знать о своих популяционных сетях и изменениях в условиях окружающей среды. Эта информация очень полезна также для понимания здоровья человека. Ясно, что NGS принесло множество преимуществ и решений для медицины и других областей, таких как сельское хозяйство, которые помогли повысить качество и производительность [34,150,151,152]. Однако это также принесло новые проблемы. 

Первая проблема связана с затратами на секвенирование. Хотя это правда, что общие затраты на NGS по сравнению с последовательностью ген за геном при секвенировании по Сэнгеру, эксперимент NGS стоит недешево и все же не доступен для всех лабораторий. Приобретение машины для секвенирования требует высоких начальных затрат, которые могут составлять от тысяч до ста тысяч евро в зависимости от типа машины, плюс расходные материалы и реагенты. Также необходимо учитывать затраты на экспериментальный план, сбор образцов и подготовку библиотеки для секвенирования. Более того, многократные затраты на разработку конвейеров секвенирования и разработку биоинформатических инструментов для улучшения этих конвейеров и выполнения последующего анализа последовательности, а также затраты на управление данными, информационное оборудование и последующий анализ данных не учитываются в общие затраты на NGS. Типичный файл BAM из одного опыта WES занимает до 30 ГБ пространства, поэтому для хранения и анализа данных нескольких пациентов требуются более высокие вычислительные мощности, а также место для хранения, что, несомненно, требует значительных затрат. Кроме того, для анализа данных могут потребоваться специалисты по биоинформатике, особенно при работе с проектами WGS. Эти дополнительные расходы, очевидно, являются частью рабочего процесса NGS и должны учитываться (для дальнейшего чтения по этой теме мы рекомендуем [153,154]). 

Обеспокоенность по поводу совместного использования данных и конфиденциальности может также возникнуть в связи с огромным объемом данных, которые генерируются с помощью анализа NGS. Спорный вопрос, какая степень защиты должна быть принята для геномных данных, должны ли геномные данные использоваться или не передаваться между несколькими сторонами (включая сотрудников лаборатории, биоинформатиков, исследователей, клиницистов, пациентов и членов их семей)? Это очень важный вопрос, особенно с точки зрения клинициста [155]. 

При анализе данных NGS важно осознавать его технические ограничения, как подчеркивается в этом документе, а именно смещение амплификации ПЦР (значительный источник смещения из-за случайных ошибок, которые могут быть внесены) и ошибки секвенирования, и, следовательно, требуется высокий охват для понять, какие варианты верны, а какие вызваны ошибками секвенирования или ПЦР. Кроме того, ограничения также существуют в последующем анализе в качестве примера выравнивания / отображения считывания, особенно для вложений, в которых некоторые инструменты выравнивания имеют низкие возможности обнаружения или не обнаруживают вообще [156]. Помимо биоинформатических инструментов, которые помогли и сделали анализ данных более автоматическим, часто требуется ручная проверка вариантов в файле BAM (рис. 7). Таким образом, очень важно понимать ограничения собственной платформы NGS и рабочего процесса, чтобы попытаться преодолеть эти ограничения и повысить качество обнаружения вариантов. 
Внешний файл, содержащий изображение, иллюстрацию и т. Д. Имя объекта: jcm-09-00132-g007.jpg 
Открыть в отдельном окне 
Рисунок 7 

Визуальная проверка файла BAM (двоичная карта выравнивания). Два примера ситуаций, которые можно наблюдать во время этой проверки. (A) Демонстрирует случай истинно положительного INDEL, подтвержденного секвенированием по Сэнгеру. Напротив, (B) показывает четкий пример ложноположительного результата, когда вариант присутствует только при обратном считывании, как позже продемонстрировано секвенированием по Сэнгеру, это технический артефакт и его следует исключить из дальнейшего анализа. 

Еще одна серьезная проблема для клиницистов и исследователей заключается в том, чтобы сопоставить результаты с соответствующей медицинской информацией, что может быть не более простой задачей, особенно при работе с новыми вариантами или новыми генами, ранее не связанными с заболеванием, что требует дополнительных усилий для подтверждения патогенности. вариантов (что в клинических условиях может оказаться невозможным). Что еще более важно, и врачи, и пациенты должны четко осознавать, что положительный результат, хотя и дает ответ, который часто завершает долгий и дорогостоящий диагностический путь, не обязательно означает, что будет предложено лучшее лечение или что будет возможно найти вылечить, следовательно, все еще во многих случаях, что генетическая информация не изменит прогноз или исход для пораженного человека [157]. Это неудобная и жесткая истина, которую клиницисты должны четко объяснять пациентам. Тем не менее, были приложены огромные усилия для увеличения выбора лучших терапевтических вариантов, основанных на результатах секвенирования ДНК, а именно для рака [158] и растущего числа редких заболеваний [144, 159]. 
Идти к: 
6. Выводы 

В заключение, несмотря на все достигнутые к настоящему времени достижения, предстоит долгий путь, прежде чем генетика сможет дать окончательный ответ на диагноз всех генетических заболеваний. Требуются дальнейшие улучшения в платформах секвенирования и стратегии обработки данных, чтобы уменьшить количество ошибок и повысить качество обнаружения вариантов. В настоящее время широко признано, что для улучшения нашего понимания болезни, особенно сложных и разнородных заболеваний, ученым и клиницистам придется объединить информацию из нескольких источников -омиков (таких как геном, транскриптом, протеом и эпигеном). Таким образом, NGS быстро развивается, чтобы иметь дело не только с классическим геномным подходом, но и быстро получить широкое применение [160, 161]. Однако одна из основных проблем - иметь дело со всеми отдельными слоями информации и интерпретировать их. Современные вычислительные методы могут быть не в состоянии обрабатывать и извлекать весь потенциал создаваемых больших наборов геномных и эпигеномных данных. Прежде всего (био) информатики, ученые и клиницисты должны будут работать вместе, чтобы интерпретировать данные и разрабатывать новые инструменты для анализа на уровне интегрированных систем. Мы считаем, что алгоритмы машинного обучения, а именно нейронные сети и вспомогательные векторные машины, а также новые разработки в области искусственного интеллекта будут иметь решающее значение для улучшения платформ и программного обеспечения NGS, которые помогут ученым и клиницистам решать сложные биологические задачи, улучшая тем самым клиническую практику. диагностика и открытие новых возможностей для разработки новых методов лечения. 
Идти к: 
Благодарности 

Авторы выражают признательность за поддержку: (i) Fundação para a Ciência e Tecnologia (FCT) [Ссылка на грант: PD / BD / 105767/2014] (R.P.). 
